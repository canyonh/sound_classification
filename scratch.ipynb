{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Log:\n",
    "* 2017/08/21: 3 layer NN not working well with raw wave input (random offset + random gain), trying again with everyhing\n",
    "  the same except only with samples with random gain\n",
    "* 2017/08/22: Fix a bug and currently 3 layer NN with 500 hidden layer has testing accuracy of 96.8%:\n",
    "  When separating data into train, validate and test set need to shuffle first so that each piece of data\n",
    "  have roughly samples with every classes in it.\n",
    "* 2017/08/22: Tried to catgorize combinations of 7 sounds but failed to generalize (correct rate is 0.011 which is odd)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "from scipy.special import comb\n",
    "import scipy.io.wavfile\n",
    "import math\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# directories\n",
    "work_dir = os.getcwd()\n",
    "source_dir = os.path.join(work_dir, \"audio_data\")\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "fdomain_dir = os.path.join(work_dir, \"fdomain\")\n",
    "\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "\n",
    "if not os.path.exists(fdomain_dir):\n",
    "    os.mkdir(fdomain_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Utilities to organize and generate samples\n",
    "#\n",
    "\n",
    "def PrepareDataSetsFDomain(directory, train_div, val_div):\n",
    "    \n",
    "    # load 7 class samples, convert to one-hot encoding\n",
    "    full_data, full_label = PrepareDataFDomain(directory)\n",
    "    print(\"full_data\", full_data.shape, \"full_label\", full_label.shape)\n",
    "\n",
    "    num_total = full_data.shape[0]\n",
    "    num_dim = full_data.shape[1]\n",
    "    assert num_total == full_label.shape[0]\n",
    "\n",
    "    #separate 65% training set, 15% validation set, %20 testing set\n",
    "    num_train = int(num_total * train_div)\n",
    "    num_val = int(num_total * val_div)\n",
    "    num_test = int(num_total - num_train - num_val)\n",
    "\n",
    "    training_set = full_data[:num_train]\n",
    "    validation_set = full_data[num_train:num_train + num_val]\n",
    "    test_set = full_data[num_train + num_val:]    \n",
    "    print(\"training_set\", training_set.shape, \"validation_set\", validation_set.shape, \"test_set\", test_set.shape)\n",
    "    \n",
    "    training_label = full_label[:num_train]\n",
    "    validation_label = full_label[num_train:num_train + num_val]\n",
    "    test_label = full_label[num_train + num_val:]    \n",
    "    print(\"training_label\", training_label.shape, \"validation_label\", validation_label.shape, \"testing_label\", test_label.shape)    \n",
    "    return training_set, training_label, validation_set, validation_label, test_set, test_label\n",
    "\n",
    "def PrepareDataSetsTDomain(directory, train_div, val_div):\n",
    "    \n",
    "    start = time.clock()\n",
    "    # load 7 class samples, convert to one-hot encoding\n",
    "    full_data, full_label = PrepareDataTDomain(directory)\n",
    "    print(\"full_data\", full_data.shape, \"full_label\", full_label.shape)\n",
    "\n",
    "    num_total = full_data.shape[0]\n",
    "    num_dim = full_data.shape[1]\n",
    "    assert num_total == full_label.shape[0]\n",
    "\n",
    "    #separate 65% training set, 15% validation set, %20 testing set\n",
    "    num_train = int(num_total * train_div)\n",
    "    num_val = int(num_total * val_div)\n",
    "    num_test = int(num_total - num_train - num_val)\n",
    "\n",
    "    training_set = full_data[:num_train]\n",
    "    validation_set = full_data[num_train:num_train + num_val]\n",
    "    test_set = full_data[num_train + num_val:]    \n",
    "    print(\"training_set\", training_set.shape, \"validation_set\", validation_set.shape, \"test_set\", test_set.shape)\n",
    "    \n",
    "    training_label = full_label[:num_train]\n",
    "    validation_label = full_label[num_train:num_train + num_val]\n",
    "    test_label = full_label[num_train + num_val:]    \n",
    "    print(\"training_label\", training_label.shape, \"validation_label\", validation_label.shape, \"testing_label\", test_label.shape)    \n",
    "    print(\"PrepareDataSetsTDomain. time: \", time.clock() - start)\n",
    "    return training_set, training_label, validation_set, validation_label, test_set, test_label\n",
    "\n",
    "\n",
    "def OneHotEncoding(dim, val):\n",
    "    count = val.shape[0]\n",
    "    one_hot = np.zeros((count, dim))\n",
    "    one_hot[np.arange(count), val] = 1\n",
    "    return one_hot\n",
    "\n",
    "def LoadAudioFile(wave_names, sounds):            \n",
    "    files = glob.glob(os.path.join(source_dir, \"*.wav\"))\n",
    "    files.sort()\n",
    "    max_length = 0\n",
    "    for f in files:\n",
    "        sound = AudioSegment.from_file(f, format=\"wav\")\n",
    "        sounds.append(sound)\n",
    "        name = os.path.basename(f)\n",
    "        wave_names.append(name[:name.index('.')])\n",
    "        print(\"wave: %s loaded %d samples\" % (name, len(sound)))\n",
    "        max_length = max(max_length, len(sound))\n",
    "    return max_length\n",
    "\n",
    "# generate wave sample\n",
    "def GenerateRandomMixedSamples(wave_names_in, sounds_in, num, length = 6000):    \n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    src_wav_cnt = len(wave_names_in)\n",
    "            \n",
    "    #exlode mixing comnbinatin\n",
    "    total_combi = 2 ** src_wav_cnt\n",
    "    for i in range(1, total_combi):\n",
    "        for j in range(num):\n",
    "            mix = AudioSegment.silent(duration=length * 2)\n",
    "            max_start = 0\n",
    "            for k in range(src_wav_cnt):\n",
    "                if i & (1 << k) != 0:\n",
    "                    start = random.randrange(0, len(sounds_in[k]))\n",
    "                    max_start = max(max_start, start)\n",
    "                    mix = mix.overlay(sounds_in[k], position=start, loop=True)\n",
    "            # get the last of the samples\n",
    "            mix = mix[-length:]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            mix = mix.apply_gain(gain)\n",
    "            mix.export(os.path.join(sample_dir, str(i)) + \"-\" + str(j) + \".wav\", format=\"wav\")\n",
    "    return\n",
    "\n",
    "# from time domain samples to generate frequency domain samples\n",
    "def GenerateFDomainSample(source_dir, dest_dir):\n",
    "    # clean current sample\n",
    "    files = glob.glob(os.path.join(dest_dir, \"*.npy\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    start = time.clock()\n",
    "    files = glob.glob(os.path.join(source_dir, \"*.wav\"))\n",
    "    #plt.figure(figsize=(40,60))\n",
    "    files.sort()\n",
    "    file_cnt = len(files)\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        y, sr = librosa.load(file_name)\n",
    "        #plt.subplot(file_cnt, 1, i + 1)\n",
    "        #librosa.display.waveplot(y)\n",
    "        D = librosa.stft(y)\n",
    "        fdomain_file_name = os.path.join(dest_dir, os.path.splitext(os.path.basename(file_name))[0])\n",
    "        np.save(fdomain_file_name, D)\n",
    "        print(fdomain_file_name, D.shape)\n",
    "        #log_power = librosa.logamplitude(D**2, ref_power=np.max)\n",
    "        #librosa.display.specshow(log_power, x_axis='time', y_axis='log')\n",
    "        #np.save(os.path.join(tft_dir, str(i)), D)    \n",
    "        #plt.title(file_name)  \n",
    "    #plt.show()\n",
    "    print(\"time elapse: %d\", time.clock() - start)\n",
    "    \n",
    "def LoadFDomainSamples(directory):\n",
    "    files = glob.glob(os.path.join(directory, \"*.npy\"))\n",
    "    shuffle(files)\n",
    "    file_cnt = len(files)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        sample = np.load(file_name)\n",
    "        #print(sample.shape)\n",
    "        #print(os.path.basename(file_name), sample.shape)\n",
    "        y.append(ord(os.path.basename(file_name)[0]) - ord('0') - 1)\n",
    "        X.append(sample.flatten())\n",
    "    return X, y\n",
    "\n",
    "def LoadTDomainSamples(directory):\n",
    "    start = time.clock()\n",
    "    files = glob.glob(os.path.join(directory, \"*.wav\"))\n",
    "    shuffle(files)\n",
    "    file_cnt = len(files)\n",
    "    X = []\n",
    "    y = []\n",
    "    max_class = 0\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        #samples, _ = librosa.load(file_name)\n",
    "        rate, samples = scipy.io.wavfile.read(file_name)\n",
    "        #print(samples.shape)\n",
    "        #print(sample.shape)\n",
    "        #print(os.path.basename(file_name), sample.shape)\n",
    "        token = os.path.basename(file_name).split('-')\n",
    "        correct_class = int(token[0]) - 1\n",
    "        max_class = max(max_class, correct_class)\n",
    "        #print(\"file:\", file_name, \"class:\", correct_class, \"max_class\", max_class)\n",
    "        y.append(correct_class)\n",
    "        X.append(samples.flatten())\n",
    "    print(\"LoadTDomainSamples, time:\", time.clock() - start)\n",
    "    return X, y, max_class + 1\n",
    "\n",
    "    \n",
    "# plot frequency domain samples\n",
    "def PlotFDomainSamples(directory, number_to_plot):\n",
    "    # load and plot and check if they match\n",
    "    files = glob.glob(os.path.join(directory, \"*.npy\"))\n",
    "    plt.figure(figsize=(40,60))\n",
    "    files.sort()\n",
    "    file_cnt = min(len(files), number_to_plot)\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        D_loaded = np.load(file_name)\n",
    "        log_power_loaded = librosa.logamplitude(D_loaded**2, ref_power=np.max)\n",
    "        plt.subplot(file_cnt, 1, i + 1)\n",
    "        librosa.display.specshow(log_power_loaded, x_axis=\"time\", y_axis='log')\n",
    "    plt.show()\n",
    "    \n",
    "# Generate num_samples with random amplitude\n",
    "def GenerateSampleRandomAmp(wave_names_in, sounds_in, num_samples, length):\n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    samples_cnt_in = len(wave_names_in)\n",
    "    for i in range(samples_cnt_in):\n",
    "        s = sounds_in[i]\n",
    "        audio_len = len(s)\n",
    "        repeat_audio = s\n",
    "        while (len(repeat_audio) < 2 * length):\n",
    "            repeat_audio = repeat_audio + s\n",
    "            \n",
    "        for j in range(num_samples):\n",
    "            start = 0\n",
    "            end = start + length\n",
    "            new_name = os.path.join(sample_dir, wave_names_in[i] + \"-\" + str(j) + \".wav\")\n",
    "            #print(\"new_name: %s(%d), start: %d, end: %d\" % (os.path.basename(new_name), len(repeat_audio), start, end))\n",
    "            sj = repeat_audio[start:end]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            sj = sj.apply_gain(gain)\n",
    "            sj.export(new_name, format=\"wav\")\n",
    "\n",
    "    \n",
    "# generate num_samples with random start time and amplitude\n",
    "def GenerateSampleRandomAmpAndOffset(wave_names_in, sounds_in, num_samples, length):\n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    samples_cnt_in = len(wave_names_in)\n",
    "    for i in range(samples_cnt_in):\n",
    "        s = sounds_in[i]\n",
    "        audio_len = len(s)\n",
    "        repeat_audio = s\n",
    "        while (len(repeat_audio) < 2 * length):\n",
    "            repeat_audio = repeat_audio + s\n",
    "            \n",
    "        for j in range(num_samples):\n",
    "            start = random.randrange(0, audio_len)\n",
    "            end = start + length\n",
    "            new_name = os.path.join(sample_dir, wave_names_in[i] + \"-\" + str(j) + \".wav\")\n",
    "            #print(\"new_name: %s(%d), start: %d, end: %d\" % (os.path.basename(new_name), len(repeat_audio), start, end))\n",
    "            sj = repeat_audio[start:end]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            sj = sj.apply_gain(gain)\n",
    "            sj.export(new_name, format=\"wav\")\n",
    "\n",
    "def PrepareDataFDomain(directory):\n",
    "    sample, label = LoadFDomainSamples(directory);\n",
    "    assert len(sample) == len(label)\n",
    "    x = np.array(sample)\n",
    "    y = np.array(label)\n",
    "    y = OneHotEncoding(7, y)\n",
    "    #print(x.shape, y.shape, y)\n",
    "    return x, y\n",
    "\n",
    "def PrepareDataTDomain(directory):\n",
    "    start = time.clock()\n",
    "    sample, label, num_classes = LoadTDomainSamples(directory);\n",
    "    assert len(sample) == len(label)\n",
    "    x = np.array(sample)\n",
    "    y = np.array(label)\n",
    "    y = OneHotEncoding(num_classes, y)\n",
    "    print(\"PrepareDataTDomain(), time: \", time.clock() - start)\n",
    "    return x, y  \n",
    "\n",
    "#not used\n",
    "def ExplodeCombination(wave_names_in, sounds_in):\n",
    "    wave_names_out = []\n",
    "    samples_out = []\n",
    "    total_combi = 2 ** len(wave_names)\n",
    "    for i in range(1, total_combi):\n",
    "        name = None\n",
    "        mix = AudioSegment.silent(duration=max_length)\n",
    "        for j in range(len(wave_names)):\n",
    "            if i & (1 << j) != 0:\n",
    "                mix = mix.overlay(sounds[j])\n",
    "                if name == None:\n",
    "                    name = wave_names[j]\n",
    "                else:\n",
    "                    name += (\"-\" + wave_names[j])\n",
    "        assert name != None\n",
    "        #rint(name, mix, len(mix))\n",
    "        wave_names_out.append(name)\n",
    "        samples_out.append(mix)\n",
    "        #mix.export(os.path.join(output_dir, name) + \".wav\", format=\"wav\")\n",
    "    return wave_names_out, samples_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate time domain samples\n",
    "#\n",
    "\n",
    "random.seed()\n",
    "\n",
    "# random samples\n",
    "wave_names = []\n",
    "sounds = []\n",
    "max_length = 0\n",
    "max_length = LoadAudioFile(wave_names, sounds)\n",
    "print(\"max_length=%d\" % (max_length))\n",
    "GenerateSampleRandomAmpAndOffset(wave_names, sounds, 10000, 3000)\n",
    "\n",
    "# random samples with mixes\n",
    "#GenerateRandomMixedSamples(wave_names, sounds, 200, 3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train with time domain samples - linear\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "start = time.clock()\n",
    "\n",
    "# prepare data set\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSetsTDomain(sample_dir, 0.8, 0.1)\n",
    "\n",
    "print('PrepareDataSetsTDomain time:', time.clock() - start)\n",
    "\n",
    "num_train = training_set.shape[0]\n",
    "dimension = training_set.shape[1]\n",
    "num_class = training_label.shape[1]\n",
    "assert num_train == training_label.shape[0]\n",
    "\n",
    "# define model\n",
    "x = tf.placeholder(tf.float32, [None, dimension])\n",
    "W = tf.Variable(tf.zeros([dimension, num_class]))\n",
    "b = tf.Variable(tf.zeros([num_class]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# prediction\n",
    "y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "train_steps = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# initialize model\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# training\n",
    "training_cnt = training_set.shape[0]\n",
    "training_batch = 500\n",
    "current = 0\n",
    "while current < training_cnt:\n",
    "    end = min(current + training_batch, training_cnt)    \n",
    "    batch_start = time.clock()    \n",
    "    sess.run(train_steps, feed_dict={x: training_set[current:end], y_: training_label[current:end]})\n",
    "    print('batch:', current, 'total:', training_cnt)\n",
    "    current += training_batch\n",
    "print('total time:', time.clock() - start)\n",
    "\n",
    "# test\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train with time domain samples - NN\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "start = time.clock()\n",
    "\n",
    "# prepare data set\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSetsTDomain(sample_dir, 0.8, 0.1)\n",
    "\n",
    "print('PrepareDataSetsTDomain time:', time.clock() - start)\n",
    "start = time.clock()\n",
    "\n",
    "dimension = training_set.shape[1]\n",
    "assert training_set.shape[1] == dimension\n",
    "assert validation_set.shape[1] == dimension\n",
    "assert testing_set.shape[1] == dimension\n",
    "\n",
    "num_class = training_label.shape[1]\n",
    "assert training_label.shape[1] == num_class\n",
    "assert validation_label.shape[1] == num_class\n",
    "assert testing_label.shape[1] == num_class\n",
    "\n",
    "num_train = training_set.shape[0]\n",
    "assert training_set.shape[0] == training_label.shape[0]\n",
    "\n",
    "num_test = testing_set.shape[0]\n",
    "assert testing_set.shape[0] == testing_label.shape[0]\n",
    "\n",
    "num_validation = validation_set.shape[0]\n",
    "assert validation_set.shape[0] == validation_label.shape[0]\n",
    "\n",
    "hidden_1 = 500\n",
    "hidden_2 = 500\n",
    "hidden_3 = 500\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # define model\n",
    "    x  = tf.placeholder(tf.float32, [None, dimension])\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "    # NN\n",
    "    #--------\n",
    "    # layer 1\n",
    "    W_1 = tf.Variable(tf.random_normal([dimension, hidden_1]))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_1]))\n",
    "    y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "    # layer 2\n",
    "    W_2 = tf.Variable(tf.random_normal([hidden_1, hidden_2]))\n",
    "    b_2 = tf.Variable(tf.random_normal([hidden_2]))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "    # layer 3\n",
    "    W_3 = tf.Variable(tf.random_normal([hidden_2, hidden_3]))\n",
    "    b_3 = tf.Variable(tf.random_normal([hidden_3]))\n",
    "    y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "\n",
    "    #output\n",
    "    W_output = tf.Variable(tf.random_normal([hidden_3, num_class]))\n",
    "    b_output = tf.Variable(tf.random_normal([num_class]))\n",
    "    y_output = tf.matmul(y_3, W_output) + b_output\n",
    "\n",
    "    # correct label\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_output))\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3).minimize(cost)\n",
    "    \n",
    "    # initialize model\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # training\n",
    "        batch_size = 4000\n",
    "        for epoch in range(50):\n",
    "            epoch_loss = 0\n",
    "            current = 0\n",
    "            for _ in range(int(num_train/batch_size)):\n",
    "                c, _ = sess.run([cost, optimizer], feed_dict={x: training_set[current:current + batch_size], y_: training_label[current:current + batch_size]})\n",
    "                epoch_loss += c\n",
    "                current += batch_size\n",
    "            print('Epoch:', epoch,  'loss:', epoch_loss)                \n",
    "            if epoch_loss < 1.0:\n",
    "                break\n",
    "                \n",
    "        correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_, 1))\n",
    "        predict_y = tf.argmax(y_output, 1)\n",
    "        correct_y = tf.argmax(y_, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "        print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: training_set[0:10], y_: training_label[0:10]}))\n",
    "        print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "        print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: testing_set[0:10], y_: testing_label[0:10]}))\n",
    "        \n",
    "    \n",
    "print('training time:', time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train with MNIST\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "# prepare data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print('PrepareDataSetsTDomain time:', time.clock() - start)\n",
    "start = time.clock()\n",
    "\n",
    "dimension = 784\n",
    "num_class = 10\n",
    "#assert num_train == training_label.shape[0]\n",
    "\n",
    "hidden_1 = 500\n",
    "hidden_2 = 500\n",
    "hidden_3 = 500\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # define model\n",
    "    x  = tf.placeholder(tf.float32, [None, dimension])\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "    # NN\n",
    "    #--------\n",
    "    # layer 1\n",
    "    W_1 = tf.Variable(tf.random_normal([dimension, hidden_1]))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_1]))\n",
    "    y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "    # layer 2\n",
    "    W_2 = tf.Variable(tf.random_normal([hidden_1, hidden_2]))\n",
    "    b_2 = tf.Variable(tf.random_normal([hidden_2]))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "    # layer 3\n",
    "    W_3 = tf.Variable(tf.random_normal([hidden_2, hidden_3]))\n",
    "    b_3 = tf.Variable(tf.random_normal([hidden_3]))\n",
    "    y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "\n",
    "    #output\n",
    "    W_output = tf.Variable(tf.random_normal([hidden_3, num_class]))\n",
    "    b_output = tf.Variable(tf.random_normal([num_class]))\n",
    "    y_output = tf.matmul(y_3, W_output) + b_output\n",
    "\n",
    "    # correct label\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_output))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "    \n",
    "    # initialize model\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # training\n",
    "        batch_size = 100\n",
    "        for epoch in range(50):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                training_set, training_label = mnist.train.next_batch(500)\n",
    "                c, _ = sess.run([cost, optimizer], feed_dict={x: training_set, y_: training_label})\n",
    "                epoch_loss += c\n",
    "            print('Epoch loss:', epoch_loss)\n",
    "\n",
    "        # test\n",
    "        correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        testing_set, testing_label = mnist.test.next_batch(batch_size)\n",
    "        print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "        print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "    \n",
    "print('training time:', time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# training with one layer neural network - time domain\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSetsTDomain(sample_dir, 0.8, 0.1)\n",
    "\n",
    "\n",
    "train_cnt = training_set.shape[0]\n",
    "dimension = training_set.shape[1]\n",
    "num_classes = training_label.shape[1]\n",
    "assert training_cnt == training_label.shape[0]\n",
    "\n",
    "batch_size = 100\n",
    "num_layer1 = 500\n",
    "num_layer2 = 500\n",
    "training_rate = 1e-4\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, dimension])\n",
    "w_0 = tf.Variable(tf.random_normal([dimension, num_classes]))\n",
    "b_0 = tf.Variable(tf.random_normal([num_classes]))\n",
    "y_0 = tf.matmul(x, w_0) + b_0\n",
    "\n",
    "y = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_0)\n",
    "train = tf.train.AdamOptimizer(training_rate).minimize(y)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "#define label\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #training\n",
    "    start = 0\n",
    "    while start < training_cnt:\n",
    "        end = min(start + batch_size, training_cnt)\n",
    "        print(\"training start:\", start, \"end:\", end)\n",
    "        sess.run(train, feed_dict = {x:training_set[start:end], y_:training_label[start:end]})\n",
    "        start += batch_size\n",
    "        \n",
    "    train_accuracy = sess.run(accuracy, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    print(training_accuracy)\n",
    "        \n",
    "     \n",
    "    #train_accuracy = sess.run(y, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    #print(training_set[0:1].shape, training_label[0:1], y)\n",
    "    #validate\n",
    "    #train_accuracy = sess.run(accuracy, feed_dict = {x:training_set, y_:training_label})\n",
    "    #print(\"train accuracy: \", train_accuracy)\n",
    "    #test_accurracy = sess.run(accuracy, feed_dict= {x:testing_set, y_:testing_label})\n",
    "    #print(\"test accuracy: \", test_accurracy)\n",
    "    #print(\"total time:\", time.clock() - start)    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, dimension])\n",
    "    w_0 = tf.Variable(tf.random_normal([dimension, num_layer1]))\n",
    "    b_0 = tf.Variable(tf.random_normal([num_layer1]))\n",
    "    y_0 = tf.nn.relu(tf.matmul(x, w_0) + b_0)\n",
    "    \n",
    "    w_1 = tf.Variable(tf.random_normal([num_layer1, num_layer2]))\n",
    "    b_1 = tf.Variable(tf.random_normal([num_layer2]))\n",
    "    y_1 = tf.nn.relu(tf.matmul(y_0, w_1) + b_1)\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal([num_layer2, num_classes]))\n",
    "    b_2 = tf.Variable(tf.random_normal([num_classes]))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, w_2) + b_2)\n",
    "    \n",
    "    y = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_2)\n",
    "    train = tf.train.AdamOptimizer(training_rate).minimize(y)\n",
    "    \n",
    "    #correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #training\n",
    "    start = 0\n",
    "    while start < training_cnt:\n",
    "        end = min(start + batch_size, training_cnt)\n",
    "        print(\"training start:\", start, \"end:\", end)\n",
    "        sess.run(train, feed_dict = {x:training_set[start:end], y_:training_label[start:end]})\n",
    "        start += batch_size\n",
    "     \n",
    "    train_accuracy = sess.run(y, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    print(training_set[0:1].shape, training_label[0:1], y)\n",
    "    #validate\n",
    "    #train_accuracy = sess.run(accuracy, feed_dict = {x:training_set, y_:training_label})\n",
    "    #print(\"train accuracy: \", train_accuracy)\n",
    "    #test_accurracy = sess.run(accuracy, feed_dict= {x:testing_set, y_:testing_label})\n",
    "    #print(\"test accuracy: \", test_accurracy)\n",
    "    #print(\"total time:\", time.clock() - start)    \n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# train with frequency domain under development\n",
    "#\n",
    "\n",
    "# generate 7 class samples\n",
    "random.seed()\n",
    "\n",
    "#wave names and sound files\n",
    "wave_names = []\n",
    "sounds = []\n",
    "max_length = 0\n",
    "max_length = LoadAudioFile(wave_names, sounds)\n",
    "print(\"max_length=%d\" % (max_length))\n",
    "\n",
    "#generate 2 samples per wave\n",
    "GenerateSample(wave_names, sounds, 1000, 3000)\n",
    "\n",
    "#generate frequency domain sample\n",
    "GenerateFDomainSample(sample_dir, fdomain_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate real sample under development\n",
    "random.seed()\n",
    "\n",
    "\n",
    "#wave names and sound files\n",
    "wave_names = []\n",
    "sounds = []\n",
    "max_length = 0\n",
    "max_length = LoadAudioFile(wave_names, sounds)\n",
    "print(\"max_length=%d\" % (max_length))\n",
    "GenerateRandomMixedSamples(wave_names, sounds, 2)\n",
    "\n",
    "#generate frequency domain sample\n",
    "GenerateFDomainSample(sample_dir, fdomain_dir)\n",
    "\n",
    "#plot some samples\n",
    "PlotFDomainSamples(fdomain_dir, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
