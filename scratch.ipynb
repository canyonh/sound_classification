{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Log:\n",
    "* 2017/08/21: 3 layer NN not working well with raw wave input (random offset + random gain), trying again with everyhing\n",
    "  the same except only with samples with random gain\n",
    "* 2017/08/22: Fix a bug and currently 3 layer NN with 500 hidden layer has testing accuracy of 96.8%:\n",
    "  When separating data into train, validate and test set need to shuffle first so that each piece of data\n",
    "  have roughly samples with every classes in it.\n",
    "* 2017/08/22: Tried to catgorize combinations of 7 sounds but failed to generalize (correct rate is 0.011 which is odd)\n",
    "* 2017/08/23: Found the bug for the low accuracy: samples are not shuffled. Tried categorizing mixed sample but failed again\n",
    "* 2017/08/24: If limit mixing offset to 5 frames the accurracy up'd to 80%. Can try to use frequency domain samples or more complex model\n",
    "* 2017/08/25: Try using spectrogram but the data size is too big. Need to reduce it down a bit\n",
    "* 2017/08/27: Use mel spectrum as features: offset and amplitude use linear and neural network both achieves >90% accurary and less iteration\n",
    "* 2017/08/30: Use more samples (65000) and mel spectrum and fix a weight initialization bug NN can achieve 98% test accuracy\n",
    "* 2017/08/31: Need to do code cleanup to make collecting and organizing samples easier\n",
    "* 2017/09/04: system test samples don't have good results; it seems that something wrong in the conversion\n",
    "* 2017/09/06: conversion seems find since the spectrogram shows simular pattern. But It seems that the mixed samples are not recognizable (0 or 63)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "from scipy.special import comb\n",
    "import scipy.io.wavfile\n",
    "import math\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "from threading import Thread\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# directories\n",
    "work_dir = os.getcwd()import os\n",
    "import glob\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "source_dir = os.path.join(work_dir, \"audio_data\")\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "fdomain_dir = os.path.join(work_dir, \"fdomain\")\n",
    "systemtest_source_dir = os.path.join(work_dir, \"systemtest-audio-data\")\n",
    "systemtest_samples = os.path.join(work_dir, \"systemtest-samples\")\n",
    "systemtest_fdomain_samples = os.path.join(work_dir, \"systemtest-fdomain-samples\")\n",
    "\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "\n",
    "if not os.path.exists(fdomain_dir):\n",
    "    os.mkdir(fdomain_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Utilities to organize and generate samples\n",
    "#\n",
    "\n",
    "def ClipAudio(input_dir, dest_dir, length=3000):\n",
    "    for f in glob.glob(os.path.join(dest_dir, \"*.wav\")):\n",
    "        os.remove(f)\n",
    "\n",
    "    for f in glob.glob(os.path.join(input_dir, \"*.wav\")):\n",
    "        sound = AudioSegment.from_file(f, format=\"wav\")\n",
    "        clipped = sound[0:length]\n",
    "        out_file_name = os.path.join(dest_dir, os.path.splitext(os.path.basename(f))[0])\n",
    "        clipped.export(out_file_name + \".wav\")\n",
    "        print(\"src:\", f, \"dst:\", out_file_name)\n",
    "    \n",
    "def PrepareDataSets(directory, train_div, val_div, is_time_domain):   \n",
    "    start = time.time()\n",
    "    # load 7 class samples, convert to one-hot encoding    \n",
    "    full_data, full_label = PrepareData(directory, is_time_domain);\n",
    "    print(\"full_data\", full_data.shape, \"full_label\", full_label.shape)\n",
    "\n",
    "    num_total = full_data.shape[0]\n",
    "    num_dim = full_data.shape[1]\n",
    "    assert num_total == full_label.shape[0]\n",
    "\n",
    "    #separate 65% training set, 15% validation set, %20 testing set\n",
    "    num_train = int(num_total * train_div)\n",
    "    num_val = int(num_total * val_div)\n",
    "    num_test = int(num_total - num_train - num_val)\n",
    "\n",
    "    training_set = full_data[:num_train]\n",
    "    validation_set = full_data[num_train:num_train + num_val]\n",
    "    test_set = full_data[num_train + num_val:]    \n",
    "    print(\"training_set\", training_set.shape, \"validation_set\", validation_set.shape, \"test_set\", test_set.shape)\n",
    "    \n",
    "    training_label = full_label[:num_train]\n",
    "    validation_label = full_label[num_train:num_train + num_val]\n",
    "    test_label = full_label[num_train + num_val:]    \n",
    "    print(\"training_label\", training_label.shape, \"validation_label\", validation_label.shape, \"testing_label\", test_label.shape)    \n",
    "    print(\"PrepareDataSetsTDomain. time: \", time.time() - start)\n",
    "    return training_set, training_label, validation_set, validation_label, test_set, test_label\n",
    "\n",
    "\n",
    "def OneHotEncoding(dim, val):\n",
    "    count = val.shape[0]\n",
    "    one_hot = np.zeros((count, dim))\n",
    "    one_hot[np.arange(count), val] = 1\n",
    "    return one_hot\n",
    "\n",
    "def LoadAudioFile(wave_names, sounds):            \n",
    "    files = glob.glob(os.path.join(source_dir, \"*.wav\"))\n",
    "    files.sort()\n",
    "    max_length = 0\n",
    "    for f in files:\n",
    "        sound = AudioSegment.from_file(f, format=\"wav\")\n",
    "        sounds.append(sound)\n",
    "        name = os.path.basename(f)\n",
    "        wave_names.append(name[:name.index('.')])\n",
    "        #print(\"wave: %s loaded %d samples\" % (name, len(sound)))\n",
    "        max_length = max(max_length, len(sound))\n",
    "    return max_length\n",
    "\n",
    "# generate wave sample\n",
    "def GenerateRandomMixedSamples(wave_names_in, sounds_in, num, length = 3000):    \n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    src_wav_cnt = len(wave_names_in)\n",
    "            \n",
    "    #exlode mixing comnbinatin\n",
    "    total_combi = 2 ** src_wav_cnt\n",
    "    for i in range(1, total_combi):\n",
    "        print(\"GenerateRandomMixedSamples(). combi:\", i);\n",
    "        for j in range(num):\n",
    "            mix = AudioSegment.silent(duration=length * 2)\n",
    "            max_start = 0\n",
    "            for k in range(src_wav_cnt):\n",
    "                if i & (1 << k) != 0:\n",
    "                    start = random.randrange(0, length / 2)                    \n",
    "                    #print(\"start\", start)\n",
    "                    mix = mix.overlay(sounds_in[k], position=start, loop=True)\n",
    "            #get from the end\n",
    "            mix = mix[-length:]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            mix = mix.apply_gain(gain)\n",
    "            mix.export(os.path.join(sample_dir, str(i)) + \"-\" + str(j) + \".wav\", format=\"wav\")\n",
    "            mix.close()                \n",
    "    return\n",
    "\n",
    "def ConvertTDomainToFDomain(src_files, dest_dir):\n",
    "    for file_name in src_files:\n",
    "        #print(\"ConvertTDomainToFDomain\", file_name)\n",
    "        [z,sr]  = librosa.core.load(file_name, sr=8000) \n",
    "        # @todo hack to keep array size the same\n",
    "        y = z[0:24000]\n",
    "        #print(\"shape:\", y.shape, \"sr:\", sr)\n",
    "        # @todo remove hack\n",
    "        # @todo: tune parameters\n",
    "        specto = librosa.feature.melspectrogram(y, sr=sr, n_fft=320, hop_length=160, n_mels=80) \n",
    "        #log_specto = librosa.core.logamplitude(specto) \n",
    "        #plt.figure(figsize=(12,4)) \n",
    "        #librosa.display.specshow(log_specto,sr=sr,x_axis='time', y_axis='mel', hop_length=160) \n",
    "        #plt.title('mel power spectrogram') \n",
    "        #plt.colorbar(format='%+02.0f dB') \n",
    "        #plt.tight_layout() \n",
    "        #plt.show() \n",
    "        #print specto.shape \n",
    "        #print log_specto.shape\n",
    "        fdomain_file_name = os.path.join(dest_dir, os.path.splitext(os.path.basename(file_name))[0])\n",
    "        #print(\"name:\", fdomain_file_name, \"spectro shape\", specto.shape)\n",
    "        np.save(fdomain_file_name, specto.flatten())\n",
    "\n",
    "def PlotSpectrum(wave_file):\n",
    "    y, sr = librosa.core.load(wave_file, sr=8000)\n",
    "    y = y[0:24000]\n",
    "    specto = librosa.feature.melspectrogram(y, sr=sr, n_fft=320, hop_length=160, n_mels=80) \n",
    "    log_specto = librosa.core.logamplitude(specto) \n",
    "    plt.figure(figsize=(12,4)) \n",
    "    plt.title(os.path.basename(wave_file))\n",
    "    librosa.display.specshow(log_specto,sr=sr,x_axis='time', y_axis='mel', hop_length=160) \n",
    "    plt.show()         \n",
    "\n",
    "# from time domain samples to generate frequency domain samples\n",
    "def ConvertSampleToFDomain(src_dir, dest_dir):\n",
    "    # clean current sample\n",
    "    #print(\"ConvertSampleToFDomain src:\", src_dir, \"dest:\", dest_dir)\n",
    "    files = glob.glob(os.path.join(dest_dir, \"*.npy\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    start = time.time()\n",
    "    files = glob.glob(os.path.join(src_dir, \"*.wav\"))\n",
    "    file_cnt = len(files)\n",
    "    ConvertTDomainToFDomain(files, dest_dir)\n",
    "    print(\"ConvertSampleToFDomain time elapse: %d\", time.time() - start)\n",
    "\n",
    "def LoadSamples(directory, is_time_domain):\n",
    "    start = time.time()\n",
    "    if is_time_domain:\n",
    "        files = glob.glob(os.path.join(directory, \"*.wav\"))\n",
    "    else:\n",
    "        files = glob.glob(os.path.join(directory, \"*.npy\"))\n",
    "    shuffle(files)\n",
    "    file_cnt = len(files)\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = []\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        if is_time_domain:\n",
    "            _, samples = scipy.io.wavfile.read(file_name)\n",
    "        else:\n",
    "            samples = np.load(file_name)\n",
    "        #print(samples.shape)\n",
    "        #print(os.path.basename(file_name), samples.shape)\n",
    "        token = os.path.basename(file_name).split('_')[0]\n",
    "        #print(\"token:\", token, labels)\n",
    "        correct_class = -1\n",
    "        if token in labels:\n",
    "            correct_class = labels.index(token)\n",
    "            #print(\"correct class:\", correct_class)\n",
    "        else:\n",
    "            correct_class = len(labels)\n",
    "            #print(\"new class: \", correct_class, \" label: \", token)\n",
    "            labels.append(token)            \n",
    "        y.append(correct_class)\n",
    "        X.append(samples)\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        print(\"new class: \", i, \" label: \", labels[i])\n",
    "    print(\"LoadSamples, time:\", time.time() - start)\n",
    "    return X, y, len(labels)\n",
    "\n",
    "    \n",
    "# plot frequency domain samples\n",
    "def PlotFDomainSamples(directory, number_to_plot):\n",
    "    # load and plot and check if they match\n",
    "    files = glob.glob(os.path.join(directory, \"*.npy\"))\n",
    "    plt.figure(figsize=(40,60))\n",
    "    files.sort()\n",
    "    file_cnt = min(len(files), number_to_plot)\n",
    "    for i in range(file_cnt):\n",
    "        file_name = files[i]\n",
    "        D_loaded = np.load(file_name)\n",
    "        log_power_loaded = librosa.logamplitude(D_loaded**2, ref_power=np.max)\n",
    "        plt.subplot(file_cnt, 1, i + 1)\n",
    "        librosa.display.specshow(log_power_loaded, x_axis=\"time\", y_axis='log')\n",
    "    plt.show()\n",
    "    \n",
    "# Generate num_samples with random amplitude\n",
    "def GenerateSampleRandomAmp(wave_names_in, sounds_in, num_samples, length):\n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    samples_cnt_in = len(wave_names_in)\n",
    "    for i in range(samples_cnt_in):\n",
    "        s = sounds_in[i]\n",
    "        audio_len = len(s)\n",
    "        repeat_audio = s\n",
    "        while (len(repeat_audio) < 2 * length):\n",
    "            repeat_audio = repeat_audio + s\n",
    "            \n",
    "        for j in range(num_samples):\n",
    "            start = 0\n",
    "            end = start + length\n",
    "            new_name = os.path.join(sample_dir, wave_names_in[i] + \"-\" + str(j) + \".wav\")\n",
    "            #print(\"new_name: %s(%d), start: %d, end: %d\" % (os.path.basename(new_name), len(repeat_audio), start, end))\n",
    "            sj = repeat_audio[start:end]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            sj = sj.apply_gain(gain)\n",
    "            sj.export(new_name, format=\"wav\")\n",
    "\n",
    "    \n",
    "# generate num_samples with random start time and amplitude\n",
    "def GenerateSampleRandomAmpAndOffset(wave_names_in, sounds_in, num_samples, length):\n",
    "    files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "    assert len(wave_names_in) == len(sounds_in)\n",
    "    samples_cnt_in = len(wave_names_in)\n",
    "    for i in range(samples_cnt_in):\n",
    "        s = sounds_in[i]\n",
    "        audio_len = len(s)\n",
    "        repeat_audio = s\n",
    "        while (len(repeat_audio) < 2 * length):\n",
    "            repeat_audio = repeat_audio + s\n",
    "            \n",
    "        for j in range(num_samples):\n",
    "            start = random.randrange(0, audio_len)\n",
    "            end = start + length\n",
    "            new_name = os.path.join(sample_dir, wave_names_in[i] + \"-\" + str(j) + \".wav\")\n",
    "            #print(\"new_name: %s(%d), start: %d, end: %d\" % (os.path.basename(new_name), len(repeat_audio), start, end))\n",
    "            sj = repeat_audio[start:end]\n",
    "            gain = random.randrange(-10.0, 10.0)\n",
    "            sj = sj.apply_gain(gain)\n",
    "            sj.export(new_name, format=\"wav\")\n",
    "\n",
    "def PrepareData(directory, is_time_domain):\n",
    "    start = time.time()\n",
    "    sample, label, num_classes = LoadSamples(directory, is_time_domain);\n",
    "    print(\"num_classes\", num_classes)\n",
    "    assert len(sample) == len(label)\n",
    "    x = np.array(sample)\n",
    "    y = np.array(label)\n",
    "    y = OneHotEncoding(num_classes, y)\n",
    "    print(\"PrepareData(), time: \", time.time() - start)\n",
    "    return x, y  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate time domain samples\n",
    "#\n",
    "random.seed()\n",
    "\n",
    "\n",
    "# random samples\n",
    "#wave_names = []\n",
    "#sounds = []\n",
    "#max_length = 0\n",
    "#max_length = LoadAudioFile(wave_names, sounds)\n",
    "#print(\"max_length=%d\" % (max_length))\n",
    "#ConvertSampleToFDomain(sample_dir, fdomain_dir)\n",
    "systest_set, systest_label = PrepareData(fdomain_dir, False)\n",
    "print(\"systest_set\",systest_set.shape, \"systest_label\", systest_label.shape)\n",
    "\n",
    "#PlotSpectrum(\"/home/kxhuan/Documents/code/github/sound_classification/samples-random-mix/2-0.wav\")\n",
    "#files = glob.glob(\"/home/kxhuan/Documents/code/github/sound_classification/systemtest-audio-data/*.wav\")\n",
    "files = glob.glob(os.path.join(sample_dir, \"*.wav\"))\n",
    "for i in range (10):\n",
    "    f = files[i]\n",
    "    PlotSpectrum(f)\n",
    "\n",
    "# current used samples\n",
    "#GenerateRandomMixedSamples(wave_names, sounds, 500, 3000)\n",
    "#ConvertSampleToFDomain(sample_dir, fdomain_dir)\n",
    "\n",
    "# random samples with mixes\n",
    "#GenerateSampleRandomAmpAndOffset(wave_names, sounds, 10000, 3000)\n",
    "#GenerateSampleRandomAmp(wave_names, sounds, 1, 3000)\n",
    "#GenerateRandomMixedSamples(wave_names, sounds, 2, 3000)\n",
    "#GenerateRandomMixedSamples(wave_names, sounds, 200, 3000)\n",
    "\n",
    "#ConvertSampleToFDomain(sample_dir, fdomain_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train with time domain samples - NN\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "start = time.time()\n",
    "\n",
    "# prepare data set\n",
    "#training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(sample_dir, 0.8, 0.1, True)\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(fdomain_dir, 0.8, 0.1, False)\n",
    "\n",
    "# random mix frequency domain\n",
    "#training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(\"/home/kxhuan/Documents/code/github/sound_classification/fdomain-random-mix\", 0.8, 0.1, False)\n",
    "#ystest_set, systest_label = PrepareData(systemtest_fdomain_samples, False)\n",
    "#print(\"systest_set\",systest_set.shape, \"systest_label\", systest_label.shape, \"training_set\", training_set.shape, \"training_label\", training_label.shape)\n",
    "\n",
    "# random mix time domain\n",
    "#training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(\"/home/kxhuan/Documents/code/github/sound_classification/samples-random-mix\", 0.8, 0.1, True)\n",
    "\n",
    "print('PrepareDataSetsTDomain time:', time.time() - start)\n",
    "start = time.time()\n",
    "\n",
    "dimension = training_set.shape[1]\n",
    "assert training_set.shape[1] == dimension\n",
    "assert validation_set.shape[1] == dimension\n",
    "assert testing_set.shape[1] == dimension\n",
    "assert systest_set.shape[1] == dimension\n",
    "\n",
    "num_class = training_label.shape[1]\n",
    "assert training_label.shape[1] == num_class\n",
    "assert validation_label.shape[1] == num_class\n",
    "assert testing_label.shape[1] == num_class\n",
    "assert systest_label.shape[1] == num_class\n",
    "\n",
    "num_train = training_set.shape[0]\n",
    "assert training_set.shape[0] == training_label.shape[0]\n",
    "\n",
    "num_test = testing_set.shape[0]\n",
    "assert testing_set.shape[0] == testing_label.shape[0]\n",
    "\n",
    "num_validation = validation_set.shape[0]\n",
    "assert validation_set.shape[0] == validation_label.shape[0]\n",
    "\n",
    "num_systest = systest_set.shape[0]\n",
    "assert systest_set.shape[0] == systest_label.shape[0]\n",
    "\n",
    "sd = 1 / np.sqrt(dimension)\n",
    "\n",
    "hidden_1 = 500\n",
    "hidden_2 = 500\n",
    "hidden_3 = 500\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # define model\n",
    "    x  = tf.placeholder(tf.float32, [None, dimension])\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "    # NN\n",
    "    #--------\n",
    "    # layer 1\n",
    "    W_1 = tf.Variable(tf.random_normal([dimension, hidden_1], mean = 0, stddev=sd))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_1], mean = 0, stddev=sd))\n",
    "    y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "    # layer 2\n",
    "    W_2 = tf.Variable(tf.random_normal([hidden_1, hidden_2], mean = 0, stddev=sd))\n",
    "    b_2 = tf.Variable(tf.random_normal([hidden_2], mean = 0, stddev=sd))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "    # layer 3\n",
    "    W_3 = tf.Variable(tf.random_normal([hidden_2, hidden_3], mean = 0, stddev=sd))\n",
    "    b_3 = tf.Variable(tf.random_normal([hidden_3], mean = 0, stddev=sd))\n",
    "    y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "\n",
    "    #output\n",
    "    W_output = tf.Variable(tf.random_normal([hidden_3, num_class], mean = 0, stddev=sd))\n",
    "    b_output = tf.Variable(tf.random_normal([num_class], mean = 0, stddev=sd))\n",
    "    y_output = tf.matmul(y_3, W_output) + b_output\n",
    "    \n",
    "    ''' linear\n",
    "    W = tf.Variable(tf.random_normal([dimension, num_class], mean = 0, stddev=sd))\n",
    "    b = tf.Variable(tf.random_normal([num_class], mean = 0, stddev=sd))\n",
    "    y_output = tf.matmul(x, W) + b\n",
    "    '''\n",
    "    # correct label\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_output))\n",
    "    optimizer = tf.train.AdamOptimizer(5e-4).minimize(cost)\n",
    "        \n",
    "    # initialize model\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # training\n",
    "        batch_size = 500\n",
    "        for epoch in range(80):\n",
    "            epoch_loss = 0\n",
    "            current = 0\n",
    "            for _ in range(int(num_train/batch_size)):\n",
    "                c, _ = sess.run([cost, optimizer], feed_dict={x: training_set[current:current + batch_size], y_: training_label[current:current + batch_size]})\n",
    "                epoch_loss += c\n",
    "                current += batch_size\n",
    "            print('Epoch:', epoch,  'loss:', epoch_loss)                \n",
    "            if epoch_loss < 1.0:\n",
    "                break\n",
    "                \n",
    "        correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_, 1))\n",
    "        predict_y = tf.argmax(y_output, 1)\n",
    "        correct_y = tf.argmax(y_, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "        print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: training_set[0:10], y_: training_label[0:10]}))\n",
    "        print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "        print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: testing_set[0:10], y_: testing_label[0:10]}))\n",
    "        #print(\"system test set accuracy: \", sess.run(accuracy, feed_dict={x: systest_set, y_: systest_label}))\n",
    "        #print(\"system test set dump: \", sess.run([predict_y, correct_y], feed_dict={x: systest_set, y_: systest_label}))\n",
    "    \n",
    "    \n",
    "print('training time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train with MNIST\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# prepare data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print('PrepareDataSetsTDomain time:', time.time() - start)\n",
    "start = time.time()\n",
    "\n",
    "dimension = 784\n",
    "num_class = 10\n",
    "#assert num_train == training_label.shape[0]\n",
    "\n",
    "hidden_1 = 500\n",
    "hidden_2 = 500\n",
    "hidden_3 = 500\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # define model\n",
    "    x  = tf.placeholder(tf.float32, [None, dimension])\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "    # NN\n",
    "    #--------\n",
    "    # layer 1\n",
    "    W_1 = tf.Variable(tf.random_normal([dimension, hidden_1]))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_1]))\n",
    "    y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "    # layer 2\n",
    "    W_2 = tf.Variable(tf.random_normal([hidden_1, hidden_2]))\n",
    "    b_2 = tf.Variable(tf.random_normal([hidden_2]))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "    # layer 3\n",
    "    W_3 = tf.Variable(tf.random_normal([hidden_2, hidden_3]))\n",
    "    b_3 = tf.Variable(tf.random_normal([hidden_3]))\n",
    "    y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "\n",
    "    #output\n",
    "    W_output = tf.Variable(tf.random_normal([hidden_3, num_class]))\n",
    "    b_output = tf.Variable(tf.random_normal([num_class]))\n",
    "    y_output = tf.matmul(y_3, W_output) + b_output\n",
    "\n",
    "    # correct label\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_output))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "    \n",
    "    # initialize model\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # training\n",
    "        batch_size = 100\n",
    "        for epoch in range(50):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                training_set, training_label = mnist.train.next_batch(500)\n",
    "                c, _ = sess.run([cost, optimizer], feed_dict={x: training_set, y_: training_label})\n",
    "                epoch_loss += c\n",
    "            print('Epoch loss:', epoch_loss)\n",
    "\n",
    "        # test\n",
    "        correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        testing_set, testing_label = mnist.test.next_batch(batch_size)\n",
    "        print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "        print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "    \n",
    "print('training time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# training with one layer neural network - time domain\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSetsTDomain(sample_dir, 0.8, 0.1)\n",
    "\n",
    "\n",
    "train_cnt = training_set.shape[0]\n",
    "dimension = training_set.shape[1]\n",
    "num_classes = training_label.shape[1]\n",
    "assert training_cnt == training_label.shape[0]\n",
    "\n",
    "batch_size = 100\n",
    "num_layer1 = 500\n",
    "num_layer2 = 500\n",
    "training_rate = 1e-4\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, dimension])\n",
    "w_0 = tf.Variable(tf.random_normal([dimension, num_classes]))\n",
    "b_0 = tf.Variable(tf.random_normal([num_classes]))\n",
    "y_0 = tf.matmul(x, w_0) + b_0\n",
    "\n",
    "y = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_0)\n",
    "train = tf.train.AdamOptimizer(training_rate).minimize(y)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "#define label\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #training\n",
    "    start = 0\n",
    "    while start < training_cnt:\n",
    "        end = min(start + batch_size, training_cnt)\n",
    "        print(\"training start:\", start, \"end:\", end)\n",
    "        sess.run(train, feed_dict = {x:training_set[start:end], y_:training_label[start:end]})\n",
    "        start += batch_size\n",
    "        \n",
    "    train_accuracy = sess.run(accuracy, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    print(training_accuracy)\n",
    "        \n",
    "     \n",
    "    #train_accuracy = sess.run(y, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    #print(training_set[0:1].shape, training_label[0:1], y)\n",
    "    #validate\n",
    "    #train_accuracy = sess.run(accuracy, feed_dict = {x:training_set, y_:training_label})\n",
    "    #print(\"train accuracy: \", train_accuracy)\n",
    "    #test_accurracy = sess.run(accuracy, feed_dict= {x:testing_set, y_:testing_label})\n",
    "    #print(\"test accuracy: \", test_accurracy)\n",
    "    #print(\"total time:\", time.time() - start)    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, dimension])\n",
    "    w_0 = tf.Variable(tf.random_normal([dimension, num_layer1]))\n",
    "    b_0 = tf.Variable(tf.random_normal([num_layer1]))\n",
    "    y_0 = tf.nn.relu(tf.matmul(x, w_0) + b_0)\n",
    "    \n",
    "    w_1 = tf.Variable(tf.random_normal([num_layer1, num_layer2]))\n",
    "    b_1 = tf.Variable(tf.random_normal([num_layer2]))\n",
    "    y_1 = tf.nn.relu(tf.matmul(y_0, w_1) + b_1)\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal([num_layer2, num_classes]))\n",
    "    b_2 = tf.Variable(tf.random_normal([num_classes]))\n",
    "    y_2 = tf.nn.relu(tf.matmul(y_1, w_2) + b_2)\n",
    "    \n",
    "    y = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_2)\n",
    "    train = tf.train.AdamOptimizer(training_rate).minimize(y)\n",
    "    \n",
    "    #correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #training\n",
    "    start = 0\n",
    "    while start < training_cnt:\n",
    "        end = min(start + batch_size, training_cnt)\n",
    "        print(\"training start:\", start, \"end:\", end)\n",
    "        sess.run(train, feed_dict = {x:training_set[start:end], y_:training_label[start:end]})\n",
    "        start += batch_size\n",
    "     \n",
    "    train_accuracy = sess.run(y, feed_dict = {x:training_set[0:1], y_:training_label[0:1]})\n",
    "    print(training_set[0:1].shape, training_label[0:1], y)\n",
    "    #validate\n",
    "    #train_accuracy = sess.run(accuracy, feed_dict = {x:training_set, y_:training_label})\n",
    "    #print(\"train accuracy: \", train_accuracy)\n",
    "    #test_accurracy = sess.run(accuracy, feed_dict= {x:testing_set, y_:testing_label})\n",
    "    #print(\"test accuracy: \", test_accurracy)\n",
    "    #print(\"total time:\", time.time() - start)    \n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# train with frequency domain under development\n",
    "#\n",
    "\n",
    "# generate 7 class samples\n",
    "random.seed()\n",
    "\n",
    "#wave names and sound files\n",
    "wave_names = []\n",
    "sounds = []\n",
    "max_length = 0\n",
    "max_length = LoadAudioFile(wave_names, sounds)\n",
    "print(\"max_length=%d\" % (max_length))\n",
    "\n",
    "#generate 2 samples per wave\n",
    "GenerateSample(wave_names, sounds, 1000, 3000)\n",
    "\n",
    "#generate frequency domain sample\n",
    "GenerateFDomainSample(sample_dir, fdomain_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#\n",
    "# Train with time domain samples - linear\n",
    "#\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "start = time.time()\n",
    "\n",
    "# prepare data set\n",
    "#training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(sample_dir, 0.8, 0.1, True)\n",
    "#training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(fdomain_dir, 0.8, 0.1, False)\n",
    "training_set, training_label, validation_set, validation_label, testing_set, testing_label = PrepareDataSets(\"/home/kxhuan/Documents/code/github/sound_classification/fdomain-random-mix\", 0.8, 0.1, False)\n",
    "\n",
    "print('PrepareDataSets time:', time.time() - start)\n",
    "\n",
    "num_train = training_set.shape[0]\n",
    "dimension = training_set.shape[1]\n",
    "num_class = training_label.shape[1]\n",
    "assert num_train == training_label.shape[0]\n",
    "\n",
    "# define model\n",
    "x = tf.placeholder(tf.float32, [None, dimension])\n",
    "W = tf.Variable(tf.zeros([dimension, num_class]))\n",
    "b = tf.Variable(tf.zeros([num_class]))\n",
    "\n",
    "#prediction\n",
    "y_output = tf.matmul(x, W) + b\n",
    "\n",
    "# correct label\n",
    "y_ = tf.placeholder(tf.float32, [None, num_class])\n",
    "\n",
    "# correct label\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_output))\n",
    "optimizer = tf.train.AdamOptimizer(5e-3).minimize(cost)\n",
    "\n",
    "# initialize model\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # training\n",
    "    batch_size = 500\n",
    "    for epoch in range(80):\n",
    "        epoch_loss = 0\n",
    "        current = 0\n",
    "        for _ in range(int(num_train/batch_size)):\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={x: training_set[current:current + batch_size], y_: training_label[current:current + batch_size]})\n",
    "            epoch_loss += c\n",
    "            current += batch_size\n",
    "        print('Epoch:', epoch,  'loss:', epoch_loss)                \n",
    "        if epoch_loss < 1.0:\n",
    "            break\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_, 1))\n",
    "    predict_y = tf.argmax(y_output, 1)\n",
    "    correct_y = tf.argmax(y_, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"training set accuracy: \", sess.run(accuracy, feed_dict={x: training_set, y_: training_label}))\n",
    "    print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: training_set[0:10], y_: training_label[0:10]}))\n",
    "    print(\"testing set accuracy: \", sess.run(accuracy, feed_dict={x: testing_set, y_: testing_label}))\n",
    "    print(\"testing set dump: \", sess.run([predict_y, correct_y], feed_dict={x: testing_set[0:10], y_: testing_label[0:10]}))\n",
    "\n",
    "        \n",
    "    \n",
    "print('training time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate real sample under development\n",
    "random.seed()\n",
    "\n",
    "\n",
    "#wave names and sound files\n",
    "wave_names = []\n",
    "sounds = []\n",
    "max_length = 0\n",
    "max_length = LoadAudioFile(wave_names, sounds)\n",
    "print(\"max_length=%d\" % (max_length))\n",
    "GenerateRandomMixedSamples(wave_names, sounds, 2)\n",
    "\n",
    "#generate frequency domain sample\n",
    "GenerateFDomainSample(sample_dir, fdomain_dir)\n",
    "\n",
    "#plot some samples\n",
    "PlotFDomainSamples(fdomain_dir, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
